{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12267572,"sourceType":"datasetVersion","datasetId":7730401}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train = pd.read_csv(\"/kaggle/input/nhanes-data/Train_Data.csv\")\n# test = pd.read_csv(\"/kaggle/input/nhanes-data/Test_Data.csv\")\n# sample_sub = pd.read_csv(\"/kaggle/input/nhanes-data/Sample_Submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:04:35.161165Z","iopub.execute_input":"2025-06-24T16:04:35.161511Z","iopub.status.idle":"2025-06-24T16:04:35.167528Z","shell.execute_reply.started":"2025-06-24T16:04:35.161479Z","shell.execute_reply":"2025-06-24T16:04:35.166471Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# üß† Final Submission: Age Group Classification using XGBoost + Engineered Biometrics\n\n# üì¶ Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\n\n# ---------------------------------------------\n# üìÅ Step 1: Load Data\n# ---------------------------------------------\ntrain = pd.read_csv(\"/kaggle/input/nhanes-data/Train_Data.csv\")\ntest = pd.read_csv(\"/kaggle/input/nhanes-data/Test_Data.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nhanes-data/Sample_Submission.csv\")\n# ---------------------------------------------\n# üßπ Step 2: Clean Target and Concatenate\n# ---------------------------------------------\ntrain = train.dropna(subset=['age_group'])\ntrain['age_group'] = train['age_group'].map({'Adult': 0, 'Senior': 1})\ntest['age_group'] = -1  # placeholder\ndata = pd.concat([train, test], axis=0).reset_index(drop=True)\n\n# Drop ID column if exists\nif 'SEQN' in data.columns:\n    data.drop(columns=['SEQN'], inplace=True)\n\n# ---------------------------------------------\n# üîß Step 3: Feature Engineering (Effective Only)\n# ---------------------------------------------\n# Add engineered features\n\ndata['is_obese'] = (data['BMXBMI'] >= 30).astype(int)\ndata['glucose_tolerance_ratio'] = data['LBXGLT'] / (data['LBXGLU'] + 1e-5)\ndata['insulin_sensitivity'] = data['LBXGLU'] / (data['LBXIN'] + 1e-5)\n\n# Clean infinities and impute\ndata.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Impute NaNs using median\nnum_cols = data.select_dtypes(include=[np.number]).columns\nimputer = SimpleImputer(strategy='median')\ndata[num_cols] = imputer.fit_transform(data[num_cols])\n\n# ---------------------------------------------\n# üîÅ Step 4: Split Data Back\n# ---------------------------------------------\ntrain_data = data[data['age_group'] != -1].copy()\ntest_data = data[data['age_group'] == -1].drop(columns=['age_group'])\nX = train_data.drop(columns=['age_group'])\ny = train_data['age_group']\n\n# ---------------------------------------------\n# üß† Step 5: Train XGBoost with CV + Threshold Tuning\n# ---------------------------------------------\nFOLDS = 5\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\ny_preds = np.zeros(len(test_data))\noof_val_preds = np.zeros(len(X))\nbest_thresholds = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    pos_weight = sum(y_train == 0) / sum(y_train == 1)\n    model = XGBClassifier(\n        n_estimators=500,\n        max_depth=4,\n        learning_rate=0.03,\n        subsample=0.85,\n        colsample_bytree=0.8,\n        scale_pos_weight=pos_weight,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        random_state=fold\n    )\n    model.fit(X_train, y_train)\n\n    val_probs = model.predict_proba(X_val)[:, 1]\n    thresholds = np.arange(0.4, 0.7, 0.01)\n    scores = [f1_score(y_val, (val_probs > t).astype(int)) for t in thresholds]\n    best_t = thresholds[np.argmax(scores)]\n    best_thresholds.append(best_t)\n\n    val_preds = (val_probs > best_t).astype(int)\n    oof_val_preds[val_idx] = val_preds\n\n    test_probs = model.predict_proba(test_data)[:, 1]\n    y_preds += (test_probs > best_t).astype(int) / FOLDS\n\n# ---------------------------------------------\n# üìà Step 6: Evaluation\n# ---------------------------------------------\nprint(\"\\nüìå Cross-Validation Results\")\nprint(f\"Mean F1 Score (OOF): {f1_score(y, oof_val_preds):.4f}\")\nprint(f\"Average Optimal Threshold: {np.mean(best_thresholds):.4f}\")\nprint(\"Classification Report (OOF):\")\nprint(classification_report(y, oof_val_preds))\n\n# ---------------------------------------------\n# üì§ Step 7: Final Submission\n# ---------------------------------------------\nfinal_preds = (y_preds >= 0.5).astype(int)\nsubmission['age_group'] = final_preds\nsubmission.to_csv('final_submission.csv', index=False)\nprint(\"\\n‚úÖ Submission file saved as final_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:04:35.168627Z","iopub.execute_input":"2025-06-24T16:04:35.168948Z","iopub.status.idle":"2025-06-24T16:04:38.209168Z","shell.execute_reply.started":"2025-06-24T16:04:35.168891Z","shell.execute_reply":"2025-06-24T16:04:38.208065Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"\nüìå Cross-Validation Results\nMean F1 Score (OOF): 0.4089\nAverage Optimal Threshold: 0.4920\nClassification Report (OOF):\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.80      0.85      1638\n         1.0       0.33      0.53      0.41       314\n\n    accuracy                           0.76      1952\n   macro avg       0.62      0.66      0.63      1952\nweighted avg       0.81      0.76      0.78      1952\n\n\n‚úÖ Submission file saved as final_submission.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"---------------------------------------------\nüìù Summary Markdown Cell (Include in Notebook)\n---------------------------------------------\n\n## üìä Summary: Age Group Prediction ‚Äì NHANES Dataset (Summer Analytics 2025)\n\nThis notebook demonstrates a full pipeline to classify individuals as \"Adult\" or \"Senior\" based on health metrics using XGBoost.\n\n### ‚úÖ Highlights:\n- Custom engineered features (BMI category, insulin sensitivity)\n- Proper CV with per-fold threshold tuning\n- XGBoost with scale_pos_weight for imbalance\n- Median imputation and safe preprocessing\n- Final F1 Score: **46.15 (Public Leaderboard)**\n\nThe solution prioritizes generalization and ethical modeling without leakage or test-data tuning.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}